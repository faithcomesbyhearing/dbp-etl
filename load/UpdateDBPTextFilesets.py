# UpdateDBPTextFilesets.py

# This class handles all of the special processing needed for text fileset
# 1) copying the fileset data into 


# Processing data in the following format
# bookNames/{bibleId}/BookNames.sql
# sql/{bibleId}/{filesetId}/{filesetId}_vpl.sql
# text/{bibleId}/{filesetId}/about.html
# text/{bibleId}/{filesetId}/fonts (is this always empty?)
# text/{bibleId}/{filesetId}/index (is this always empty?)
# text/{bibleId}/{filesetId}/index.html
# text/{bibleId}/{filesetId}/indexlemma (is this always empty?)
# text/{bibleId}/{filesetId}/info.json (in athenia source)
# text/{bibleId}/{filesetId}/title.json
# text/{bibleId}/{filesetId}/{filesetId}_{seqno}_{usfm}_{chapter}.html (one file per chapter)

import io
import sys
import os
from xml.dom import minidom
from Config import *


class UpdateDBPTextFilesets:

	def __init__(self, config, db, dbOut):
		self.config = config
		self.db = db
		self.dbOut = dbOut


	def copyTextFileset(self):
		print("TBD")


	def updateVerses(self, directory, hashId, bibleId, filesetId):
		versesFile = directory + "sql/" + bibleId + "/" + filesetId + "/" + filesetId + "_vpl.sql"
		# The hash_id is generated by a hard coded dbp-prod
		# This needs to be replaced with a variable name created when the fileset is created
		with open(versesFile, "r") as verses:
			for verse in verses:
				if not verse.startswith("USE "):
					print(verse)
					# replace hashId
					self.dbOut.rawStatements(verse)


	def updateBookNames(self, bibleId):
		booksFile = directory + "bookNames/" + bibleId + "/" + BookNames.sql
		doc = minidom.parse(booksFile)
		root = doc.childNodes
		if len(root) != 1 or root[0].nodeName != "BookNames":
			print ("FATAL: First node of BookNames.xml must be <BookNames>")
			sys.exit()
		else:
			for recNode in root[0].childNodes:
				print(recNode)
				#if recNode.nodeType == 1:
				#	if recNode.nodeName != "qry_dbp4_Regular_and_NonDrama":
				#		print("ERROR: Child nodes in LPTS Export must be 'qry_dbp4_Regular_and_NonDrama'")
				#		sys.exit()
				#	else:
				#		resultRow = {}
				#		for fldNode in recNode.childNodes:
				#			if fldNode.nodeType == 1:
				#				#print(fldNode.nodeName + " = " + fldNode.firstChild.nodeValue)
				#				resultRow[fldNode.nodeName] = fldNode.firstChild.nodeValue
"""
<BookNames>
  <book abbr="" alt="" code="GEN" header="STAT" long="" short=""/>
  <book abbr="" alt="" code="EXO" header="KISIM BEK" long="" short=""/>
  <book abbr="" alt="" code="LEV" header="WOK PRIS" long="" short=""/>
  <book abbr="" alt="" code="NUM" header="NAMBA" long="" short=""/>
  <book abbr="" alt="" code="DEU" header="LO" long="" short=""/>
  <book abbr="" alt="" code="JOS" header="JOSUA" long="" short=""/>
  <book abbr="" alt="" code="JDG" header="HETMAN" long="" short=""/>
  <book abbr="" alt="" code="RUT" header="RUT" long="" short=""/>
"""

	## Update table with bible books data from audio or video fileset
	def createIfAbsent(self, filesetPrefix):
		(typeCode, bibleId, filesetId) = filesetPrefix.split("/")
		if typeCode in {"audio", "video"}:
			bookCount = db.selectScalar("SELECT count(*) FROM bible_books WHERE bible_id = %s", (bibleId,))
			if bookCount == 0:
				bookIdList = []
				bookChapterMap = {}
				priorBookId = None
				priorChapter = None
				csvFilename = self.config.directory_accepted + filesetPrefix.replace("/", "_") + ".csv"
				with open(csvFilename, newline='\n') as csvfile:
					reader = csv.DictReader(csvfile)
					for row in reader:
						bookId = row["book_id"]
						chapter = row["chapter_start"]
						if bookId != priorBookId:
							bookIdList.append(bookId)
							bookChapterMap[bookId] = [chapter]
						elif chapter != priorChapter:
							chapters = bookChapterMap[bookId]
							chapters.append("," + chapter)
							bookChapterMap[bookId] = chapters
						priorBookId = bookId
						priorChapter = chapter

				insertRows = []
				for bookId in bookIdList:
					bookName = self.bookNameMap.get(bookId)
					chapters = bookChapterMap[bookId]
					insertRows.append((bookName, bookName, chapters, bibleId, bookId))

				tableName = "bible_books"
				pkeyNames = ("bible_id", "book_id")
				attrNames = ("name", "name_short", "chapters")
				self.dbOut.insert(tableName, pkeyNames, attrNames, insertRows)



if (__name__ == '__main__'):
	config = Config()
	db = SQLUtility(config)
	dbOut = SQLBatchExec(config)
	texts = UpdateDBPTextFilesets(config, db, dbOut)
	dirname = self.config.directory_database
	for typeCode in os.listdir(dirname):
		for bibleId in os.listdir(dirname + typeCode):
			for filesetId in os.listdir(dirname + typeCode + os.sep + bibleId):
				csvFilename = config.directory_accepted + typeCode + "_" + bibleId + "_" + filesetId + ".csv"
				if os.exists(csvFilename):
					if typeCode == "sql":
						hashId = "abcdefg" ### test only
						text.updateVerses(dirname, hashId, bibleId, filesetId)
					elif typeCode == "bookNames":
						text.updateBookNames(dirname, bibleId)
	dbOut.displayCounts()
	dbOut.displayStatements()
	#dbOut.execute()


"""
sys.setdefaultencoding("utf-8")

def bibleList(inDir):
	tempList = []
	os.chdir(inDir)
	cmd = os.popen('ls',"r")
	while 1:
		line = cmd.readline()
		if not line: break
		if os.path.isdir(line.strip()):
			tempList.append(line.strip())
	return tempList

textIds=["Reg_NTTextDamID1", "Reg_NTTextDamID2", "Reg_NTTextDamID3", "ND_NTTextDamID1", "ND_NTTextDamID2", "ND_NTTextDamID3", "Reg_OTTextDamID1", "Reg_OTTextDamID2", "Reg_OTTextDamID3", "ND_OTTextDamID1", "ND_OTTextDamID2", "ND_OTTextDamID3", "DBP_Equivalent"]


sourceFolder="/home/parallels/aletheia2/output/"
destinationFolder="/home/parallels/FCBH_uploader/"

bibleList=bibleList(sourceFolder)

#bibleList=["BREPUB"]

xmldoc = minidom.parse("/home/parallels/lptsmanager/qry_dbp4_Regular_and_NonDrama.xml")
nodeList = xmldoc.getElementsByTagName('qry_dbp4_Regular_and_NonDrama')

for node in nodeList:
	children=node.childNodes
	nodeName=None
	bibleIds=set()
	for child in children:
		if child.nodeName != '#text' and child.nodeName in textIds:
			#print child.nodeName+": "+child.firstChild.nodeValue
			if child.nodeName=="DBP_Equivalent":
				nodeName=child.firstChild.nodeValue
			else:
				bibleIds.add(child.firstChild.nodeValue[:-4])
	#print nodeName
	bibleIds=list(bibleIds)
	if nodeName is not None and nodeName not in ['N/A','#N/A']:
		for bibleId in bibleIds:
			if bibleId in bibleList:
				biblePath=sourceFolder+bibleId+"/"
			
				if os.path.isdir(biblePath+"dbp/"):
					mkString="mkdir -p "+destinationFolder+"text/"+nodeName
					os.system(mkString)
					formattedTextString="cp -r "+biblePath+"dbp "+destinationFolder+"text/"+nodeName+"/"
					print formattedTextString
					os.system(formattedTextString)
					mvString="mv "+destinationFolder+"text/"+nodeName+"/dbp "+destinationFolder+"text/"+nodeName+"/"+bibleId
					print mvString
					os.system(mvString)
				
				if os.path.isdir(biblePath+"sql/"):
					m = hashlib.md5()
					m.update(bibleId+"dbp-prodtext_plain")
					hash_id= m.hexdigest()[:12]
					mkString="mkdir -p "+destinationFolder+"sql/"+nodeName
					os.system(mkString)
					formattedTextString="cp -r "+biblePath+"sql "+destinationFolder+"sql/"+nodeName+"/"
					print formattedTextString
					os.system(formattedTextString)
					mvString="mv "+destinationFolder+"sql/"+nodeName+"/sql "+destinationFolder+"sql/"+nodeName+"/"+bibleId
					print mvString
					os.system(mvString)
					
					vpl_path=destinationFolder+"sql/"+nodeName+"/"+bibleId+"/"+bibleId+"_vpl.sql"
					fo=open(vpl_path,"rb")
					vplData=fo.read().strip().replace("'"+bibleId+"'","'"+hash_id+"'").replace("`database`","dbp_newdata")
					fo.close()
					
					fw=open(vpl_path,"wb")
					fw.write(vplData)
					fw.close()
					
				
				if os.path.isfile(biblePath+"BookNames.xml"):
					mkString="mkdir -p "+destinationFolder+"bookNames/"+nodeName
					os.system(mkString)
					formattedTextString="cp -r "+biblePath+"BookNames.xml "+destinationFolder+"bookNames/"+nodeName+"/BookNames.xml"
					print formattedTextString
					os.system(formattedTextString)

"""





